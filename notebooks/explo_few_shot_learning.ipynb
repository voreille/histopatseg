{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bdd7ac1",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- Check that the column name to compute histograms and stuff are correct (i.e. 'image_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75bce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from histopatseg.visualization.visualization import plot_embeddings\n",
    "from histopatseg.evaluation.utils import aggregate_tile_embeddings, custom_balanced_group_kfold\n",
    "from histopatseg.evaluation.prototype_classifier import PrototypeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c3d60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = Path(\".\").resolve().parent\n",
    "print(f\"Project Directory: {project_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d907263a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file = project_dir / \"data/processed/embeddings/lunghist700_20x_UNI2_embeddings.npz\"\n",
    "metadata  = pd.read_csv(project_dir / \"/home/valentin/workspaces/histopatseg/data/processed/LungHist700_tiled/LungHist700_20x/metadata.csv\").set_index(\"tile_id\")\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04591c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embeddings\n",
    "data = np.load(embedding_file)\n",
    "embeddings = data[\"embeddings\"]\n",
    "tile_ids = data[\"tile_ids\"]\n",
    "embedding_dim = data[\"embedding_dim\"]\n",
    "\n",
    "# Check if all embedding tile_ids are in the metadata index\n",
    "missing_ids = [id for id in tile_ids if id not in metadata.index]\n",
    "if missing_ids:\n",
    "    print(f\"Warning: {len(missing_ids)} tile_idembeddings = remove_image_pcs_for_normalized(embeddings, image_ids, n_components=8)s from embeddings are not in metadata\")\n",
    "    print(f\"First few missing IDs: {missing_ids[:5]}\")\n",
    "\n",
    "metadata = metadata.reindex(tile_ids)\n",
    "metadata[\"subclass\"] = metadata.apply(\n",
    "    lambda row: row[\"superclass\"]\n",
    "    if pd.isna(row[\"subclass\"]) and row[\"superclass\"] == \"nor\"\n",
    "    else row[\"subclass\"],\n",
    "    axis=1,\n",
    ")\n",
    " \n",
    "\n",
    "# Print basic information\n",
    "print(f\"Loaded {len(embeddings)} embeddings with dimensionality {embeddings.shape[1]}\")\n",
    "print(f\"Embedding dimension from model: {embedding_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4ae7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = normalize(embeddings, norm=\"l2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c568d12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class OrthogonalDeconfounding(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Remove group-specific information by projection to orthogonal space.\n",
    "    \n",
    "    This class can be used to remove batch effects, patient-specific,\n",
    "    or image-specific information from embeddings by projecting them\n",
    "    onto a space orthogonal to the directions that predict group membership.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, C=1.0):\n",
    "        \"\"\"\n",
    "        Initialize the deconfounding transformer.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        C : float\n",
    "            Regularization parameter for LogisticRegression (controls the strength\n",
    "            of L2 regularization). Lower values encourage more regularization.\n",
    "        \"\"\"\n",
    "        self.C = C\n",
    "        self.is_fitted_ = False\n",
    "    \n",
    "    def fit(self, X, y=None, groups=None):\n",
    "        \"\"\"\n",
    "        Fit the group predictor and compute orthogonal space.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Embedding vectors\n",
    "        y : array-like, default=None\n",
    "            Not used, present for API consistency\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group identifiers (e.g., patient_ids, image_ids)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\"groups must be provided\")\n",
    "            \n",
    "        # Create matrix to hold directions we'll project out\n",
    "        n_features = X.shape[1]\n",
    "        self.group_directions = np.zeros((0, n_features))\n",
    "        \n",
    "        # For each group, find the direction in embedding space that best separates\n",
    "        # that group from others (one-vs-rest approach)\n",
    "        unique_groups = np.unique(groups)\n",
    "        print(f\"Identifying predictive directions for {len(unique_groups)} groups\")\n",
    "        \n",
    "        for group in unique_groups:\n",
    "            # Create a binary task: this group vs all others\n",
    "            binary_task = (groups == group).astype(int)\n",
    "            \n",
    "            # Train a linear model for this task\n",
    "            model = LogisticRegression(penalty='l2', C=self.C, solver='lbfgs', max_iter=1000)\n",
    "            model.fit(X, binary_task)\n",
    "            \n",
    "            # Extract the coefficient vector and normalize\n",
    "            direction = model.coef_[0]\n",
    "            norm = np.linalg.norm(direction)\n",
    "            \n",
    "            # Skip if direction is close to zero (can happen with high regularization)\n",
    "            if norm < 1e-10:\n",
    "                continue\n",
    "                \n",
    "            direction = direction / norm\n",
    "            \n",
    "            # Add to our collection of directions\n",
    "            self.group_directions = np.vstack([self.group_directions, direction])\n",
    "        \n",
    "        # Orthogonalize the directions using QR decomposition\n",
    "        # This is more stable than individual projections\n",
    "        q, r = np.linalg.qr(self.group_directions.T)\n",
    "        self.orthogonal_basis = q  # Store the full orthogonal basis\n",
    "        \n",
    "        self.n_available_directions = min(len(unique_groups), n_features)\n",
    "        print(f\"Computed {self.n_available_directions} orthogonal directions (use transform with n_components to select)\")\n",
    "        \n",
    "        self.is_fitted_ = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, n_components=None):\n",
    "        \"\"\"\n",
    "        Project embeddings to space orthogonal to group-predictive directions.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Embedding vectors to deconfound\n",
    "        n_components : int or None\n",
    "            Number of components to use for deconfounding. If None, will use all available.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        X_projected : array-like of shape (n_samples, n_features)\n",
    "            Deconfounded embeddings\n",
    "        \"\"\"\n",
    "        if not self.is_fitted_:\n",
    "            raise ValueError(\"This OrthogonalDeconfounding instance is not fitted yet. Call 'fit' first.\")\n",
    "            \n",
    "        # Determine number of components to use\n",
    "        if n_components is None:\n",
    "            n_directions = self.n_available_directions\n",
    "        else:\n",
    "            n_directions = min(n_components, self.n_available_directions)\n",
    "            \n",
    "        print(f\"Using {n_directions} orthogonal directions for deconfounding\")\n",
    "        \n",
    "        # Get the selected number of orthogonal directions\n",
    "        selected_directions = self.orthogonal_basis[:, :n_directions]\n",
    "        \n",
    "        # Create the projection matrix for the orthogonal complement\n",
    "        projection_matrix = np.eye(X.shape[1]) - selected_directions @ selected_directions.T\n",
    "        \n",
    "        # Project the embeddings\n",
    "        X_projected = X @ projection_matrix\n",
    "        \n",
    "        # Re-normalize to unit length\n",
    "        X_projected = normalize(X_projected, norm='l2')\n",
    "        \n",
    "        return X_projected\n",
    "\n",
    "deconfounder = OrthogonalDeconfounding(C=0.01)\n",
    "deconfounder.fit(embeddings, groups=metadata[\"patient_id\"].values)\n",
    "embeddings = deconfounder.transform(embeddings, n_components=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ca3b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_aca = metadata[\"superclass\"] == \"aca\"\n",
    "metadata = metadata[mask_aca]\n",
    "embeddings = embeddings[mask_aca]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ff8696",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = metadata[\"class_name\"].values\n",
    "groups = metadata[\"patient_id\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740f1095",
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = np.linalg.norm(embeddings, axis=1)\n",
    "print(\"Mean norm:\", norms.mean())\n",
    "print(\"Min norm:\", norms.min(), \"Max norm:\", norms.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7f0001",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = list(custom_balanced_group_kfold(\n",
    "    embeddings,\n",
    "    y,\n",
    "    groups,\n",
    "    n_splits=4,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74cc419",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a15e38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, test_idx = cv[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525403d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_image_pcs_for_normalized(embeddings, image_ids, n_components=2):\n",
    "    \"\"\"\n",
    "    Remove principal components that capture image-level variation,\n",
    "    specially adapted for L2-normalized embeddings.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    embeddings : numpy array of shape (n_samples, n_features)\n",
    "        L2-normalized embedding vectors\n",
    "    image_ids : numpy array of shape (n_samples,)\n",
    "        Image ID for each tile\n",
    "    n_components : int\n",
    "        Number of principal components to remove\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    corrected_embeddings : numpy array of shape (n_samples, n_features)\n",
    "        Embeddings with image-level PCs removed\n",
    "    \"\"\"\n",
    "    unique_image_ids = np.unique(image_ids)\n",
    "    \n",
    "    # Compute image means\n",
    "    image_means = np.zeros((len(unique_image_ids), embeddings.shape[1]))\n",
    "    for i, img_id in enumerate(unique_image_ids):\n",
    "        mask = image_ids == img_id\n",
    "        # For L2-normalized vectors, take the mean and re-normalize\n",
    "        mean_vector = embeddings[mask].mean(axis=0)\n",
    "        image_means[i] = mean_vector / np.linalg.norm(mean_vector)\n",
    "    \n",
    "    # Compute PCA on image means to identify image-specific directions\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(image_means)\n",
    "    image_pcs = pca.components_\n",
    "    \n",
    "    # For each embedding, remove projection onto image PCs\n",
    "    corrected_embeddings = embeddings.copy()\n",
    "    for i in range(len(embeddings)):\n",
    "        embedding = embeddings[i]\n",
    "        \n",
    "        # Remove projections onto image PCs\n",
    "        for pc in image_pcs:\n",
    "            # Calculate projection\n",
    "            proj = np.dot(embedding, pc) * pc\n",
    "            # Subtract projection\n",
    "            embedding = embedding - proj\n",
    "            \n",
    "        # Renormalize to unit length\n",
    "        corrected_embeddings[i] = embedding / np.linalg.norm(embedding)\n",
    "    \n",
    "    return corrected_embeddings\n",
    "\n",
    "# Apply to your normalized embeddings\n",
    "image_ids = metadata[\"image_id\"].values\n",
    "embeddings = remove_image_pcs_for_normalized(embeddings, image_ids, n_components=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23065c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_elbow_method(embeddings, max_clusters=20):\n",
    "    \"\"\"Plot the elbow method to find optimal number of clusters.\"\"\"\n",
    "    inertia = []\n",
    "    k_range = range(1, max_clusters + 1)\n",
    "    \n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=\"auto\")\n",
    "        kmeans.fit(embeddings)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "    \n",
    "    # Plot the elbow\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(k_range, inertia, 'o-', markersize=8)\n",
    "    plt.xlabel('Number of Clusters (k)')\n",
    "    plt.ylabel('Inertia (Within-Cluster Sum of Squares)')\n",
    "    plt.title('Elbow Method for Optimal k')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Calculate the angle/second derivative to find the elbow point\n",
    "    angles = []\n",
    "    for i in range(1, len(inertia)-1):\n",
    "        x1, y1 = k_range[i-1], inertia[i-1]\n",
    "        x2, y2 = k_range[i], inertia[i]\n",
    "        x3, y3 = k_range[i+1], inertia[i+1]\n",
    "        \n",
    "        # Calculate the angle between the two line segments\n",
    "        angle = np.abs(np.arctan2(y3-y2, x3-x2) - np.arctan2(y2-y1, x2-x1))\n",
    "        angles.append((k_range[i], angle))\n",
    "    \n",
    "    # Find the point with maximum angle\n",
    "    elbow_point = max(angles, key=lambda x: x[1])[0]\n",
    "    plt.axvline(x=elbow_point, color='r', linestyle='--', label=f'Elbow point: k={elbow_point}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return elbow_point\n",
    "\n",
    "# Normalize embeddings before clustering\n",
    "embeddings_norm = normalize(embeddings, norm=\"l2\")\n",
    "elbow_k = plot_elbow_method(embeddings_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10e6151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit k-means\n",
    "n_clusters = 8  # you can tune this (e.g., 5-8 for LUAD patterns)\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=\"auto\")\n",
    "cluster_labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# Store cluster assignments in metadata\n",
    "metadata[\"cluster\"] = cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e766f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d699594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Get LUAD metadata for training and test\n",
    "metadata_train = metadata.iloc[train_idx]\n",
    "metadata_test = metadata.iloc[test_idx]\n",
    "\n",
    "\n",
    "# Helper function to compute histograms per WSI\n",
    "def compute_wsi_histograms(meta, n_clusters):\n",
    "    wsi_histograms = {}\n",
    "    wsi_labels = {}\n",
    "\n",
    "    for image_id, group in meta.groupby(\"image_id\"):\n",
    "        cluster_counts = np.bincount(group[\"cluster\"], minlength=n_clusters)\n",
    "        histogram = cluster_counts / cluster_counts.sum()  # normalize\n",
    "        wsi_histograms[image_id] = histogram\n",
    "        wsi_labels[image_id] = group[\"class_name\"].iloc[0]  # assuming consistent label\n",
    "\n",
    "    return wsi_histograms, wsi_labels\n",
    "\n",
    "# Compute histograms\n",
    "train_histograms, train_labels = compute_wsi_histograms(metadata_train, n_clusters)\n",
    "test_histograms, test_labels = compute_wsi_histograms(metadata_test, n_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dbfd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Convert dicts to arrays\n",
    "X_train = np.stack(list(train_histograms.values()))\n",
    "y_train = np.array(list(train_labels.values()))\n",
    "\n",
    "X_test = np.stack(list(test_histograms.values()))\n",
    "y_test = np.array(list(test_labels.values()))\n",
    "\n",
    "# Train logistic regression\n",
    "clf = LogisticRegression(multi_class=\"multinomial\", max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f8aab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dicts to arrays\n",
    "X_train = embeddings[train_idx]\n",
    "y_train = y[train_idx]\n",
    "\n",
    "X_test = embeddings[test_idx]\n",
    "y_test = y[test_idx]\n",
    "\n",
    "# Train logistic regression\n",
    "clf = LogisticRegression(max_iter=1000, penalty=\"l1\", C=10, solver=\"saga\")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3743912c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchpl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

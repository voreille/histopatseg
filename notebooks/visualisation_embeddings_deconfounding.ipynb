{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c06a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "import umap\n",
    "\n",
    "from histopatseg.visualization.visualization import plot_embeddings\n",
    "from histopatseg.evaluation.utils import aggregate_tile_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6807a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = Path(\".\").resolve().parent\n",
    "print(f\"Project Directory: {project_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14d276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file = project_dir / \"data/processed/embeddings/lunghist700_20x_UNI2_embeddings.npz\"\n",
    "metadata  = pd.read_csv(project_dir / \"data/processed/LungHist700_tiled/LungHist700_20x/metadata.csv\").set_index(\"tile_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63036799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embeddings\n",
    "data = np.load(embedding_file)\n",
    "embeddings = data[\"embeddings\"]\n",
    "tile_ids = data[\"tile_ids\"]\n",
    "embedding_dim = data[\"embedding_dim\"]\n",
    "\n",
    "# Print basic information\n",
    "print(f\"Loaded {len(embeddings)} embeddings with dimensionality {embeddings.shape[1]}\")\n",
    "print(f\"Embedding dimension from model: {embedding_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95682d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all embedding tile_ids are in the metadata index\n",
    "missing_ids = [id for id in tile_ids if id not in metadata.index]\n",
    "if missing_ids:\n",
    "    print(f\"Warning: {len(missing_ids)} tile_ids from embeddings are not in metadata\")\n",
    "    print(f\"First few missing IDs: {missing_ids[:5]}\")\n",
    "metadata = metadata.reindex(tile_ids)\n",
    "metadata[\"image_id\"] = metadata[\"image_id\"].astype(str) + \"__\" + metadata[\"patient_id\"].astype(str)\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbee4bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = normalize(embeddings, norm=\"l2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c1738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = np.linalg.norm(embeddings, axis=1)\n",
    "print(\"Mean norm:\", norms.mean())\n",
    "print(\"Min norm:\", norms.min(), \"Max norm:\", norms.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ff5177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embeddings(embeddings, metadata, method=\"t-SNE\", aggregated=False):\n",
    "    \"\"\"Generate visualizations for embeddings using specified dimensionality reduction.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: The embedding vectors\n",
    "        metadata: Associated metadata\n",
    "        method: Dimensionality reduction method (\"t-SNE\", \"UMAP\", or \"PCA\")\n",
    "        aggregated: Whether these are aggregated embeddings\n",
    "    \"\"\"\n",
    "    suffix = \"with Mean aggregation\" if aggregated else \"without Aggregation\"\n",
    "    \n",
    "    # Perform dimensionality reduction\n",
    "    if method == \"t-SNE\":\n",
    "        reducer = TSNE(\n",
    "            n_components=2,\n",
    "            perplexity=15 if aggregated else 30,\n",
    "            n_iter=1000,\n",
    "            random_state=42,\n",
    "            init='pca'\n",
    "        )\n",
    "    elif method == \"UMAP\":\n",
    "        reducer = umap.UMAP(\n",
    "            n_neighbors=10 if aggregated else 15,\n",
    "            min_dist=0.2 if aggregated else 0.1,\n",
    "            n_components=2,\n",
    "            metric='euclidean',\n",
    "            random_state=42\n",
    "        )\n",
    "    elif method == \"PCA\":\n",
    "        reducer = PCA(n_components=2, random_state=42)\n",
    "    \n",
    "    reduced_data = reducer.fit_transform(embeddings)\n",
    "    \n",
    "    # Plot with different colorings\n",
    "    for color_by in ['class_name', 'superclass', 'patient_id']:\n",
    "        fig = plot_embeddings(\n",
    "            reduced_data=reduced_data,\n",
    "            metadata=metadata,\n",
    "            color_by=color_by,\n",
    "            method_name=method,\n",
    "            title=f'{method} Projection of LungHist700 Embeddings {suffix}',\n",
    "            palette_name='tab10'\n",
    "        )\n",
    "        plt.show()\n",
    "    \n",
    "    return reduced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686af66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_embedding = visualize_embeddings(embeddings, metadata, \"t-SNE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44218743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class OrthogonalDeconfounding(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Remove group-specific information by projection to orthogonal space.\n",
    "    \n",
    "    This class can be used to remove batch effects, patient-specific,\n",
    "    or image-specific information from embeddings by projecting them\n",
    "    onto a space orthogonal to the directions that predict group membership.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, C=1.0):\n",
    "        \"\"\"\n",
    "        Initialize the deconfounding transformer.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        C : float\n",
    "            Regularization parameter for LogisticRegression (controls the strength\n",
    "            of L2 regularization). Lower values encourage more regularization.\n",
    "        \"\"\"\n",
    "        self.C = C\n",
    "        self.is_fitted_ = False\n",
    "    \n",
    "    def fit(self, X, y=None, groups=None):\n",
    "        \"\"\"\n",
    "        Fit the group predictor and compute orthogonal space.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Embedding vectors\n",
    "        y : array-like, default=None\n",
    "            Not used, present for API consistency\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group identifiers (e.g., patient_ids, image_ids)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\"groups must be provided\")\n",
    "            \n",
    "        # Create matrix to hold directions we'll project out\n",
    "        n_features = X.shape[1]\n",
    "        self.group_directions = np.zeros((0, n_features))\n",
    "        \n",
    "        # For each group, find the direction in embedding space that best separates\n",
    "        # that group from others (one-vs-rest approach)\n",
    "        unique_groups = np.unique(groups)\n",
    "        print(f\"Identifying predictive directions for {len(unique_groups)} groups\")\n",
    "        \n",
    "        for group in unique_groups:\n",
    "            # Create a binary task: this group vs all others\n",
    "            binary_task = (groups == group).astype(int)\n",
    "            \n",
    "            # Train a linear model for this task\n",
    "            model = LogisticRegression(penalty='l2', C=self.C, solver='lbfgs', max_iter=1000)\n",
    "            model.fit(X, binary_task)\n",
    "            \n",
    "            # Extract the coefficient vector and normalize\n",
    "            direction = model.coef_[0]\n",
    "            norm = np.linalg.norm(direction)\n",
    "            \n",
    "            # Skip if direction is close to zero (can happen with high regularization)\n",
    "            if norm < 1e-10:\n",
    "                continue\n",
    "                \n",
    "            direction = direction / norm\n",
    "            \n",
    "            # Add to our collection of directions\n",
    "            self.group_directions = np.vstack([self.group_directions, direction])\n",
    "        \n",
    "        # Orthogonalize the directions using QR decomposition\n",
    "        # This is more stable than individual projections\n",
    "        q, r = np.linalg.qr(self.group_directions.T)\n",
    "        self.orthogonal_basis = q  # Store the full orthogonal basis\n",
    "        \n",
    "        self.n_available_directions = min(len(unique_groups), n_features)\n",
    "        print(f\"Computed {self.n_available_directions} orthogonal directions (use transform with n_components to select)\")\n",
    "        \n",
    "        self.is_fitted_ = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, n_components=None):\n",
    "        \"\"\"\n",
    "        Project embeddings to space orthogonal to group-predictive directions.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Embedding vectors to deconfound\n",
    "        n_components : int or None\n",
    "            Number of components to use for deconfounding. If None, will use all available.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        X_projected : array-like of shape (n_samples, n_features)\n",
    "            Deconfounded embeddings\n",
    "        \"\"\"\n",
    "        if not self.is_fitted_:\n",
    "            raise ValueError(\"This OrthogonalDeconfounding instance is not fitted yet. Call 'fit' first.\")\n",
    "            \n",
    "        # Determine number of components to use\n",
    "        if n_components is None:\n",
    "            n_directions = self.n_available_directions\n",
    "        else:\n",
    "            n_directions = min(n_components, self.n_available_directions)\n",
    "            \n",
    "        print(f\"Using {n_directions} orthogonal directions for deconfounding\")\n",
    "        \n",
    "        # Get the selected number of orthogonal directions\n",
    "        selected_directions = self.orthogonal_basis[:, :n_directions]\n",
    "        \n",
    "        # Create the projection matrix for the orthogonal complement\n",
    "        projection_matrix = np.eye(X.shape[1]) - selected_directions @ selected_directions.T\n",
    "        \n",
    "        # Project the embeddings\n",
    "        X_projected = X @ projection_matrix\n",
    "        \n",
    "        # Re-normalize to unit length\n",
    "        X_projected = normalize(X_projected, norm='l2')\n",
    "        \n",
    "        return X_projected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773e0c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_image_pcs_for_normalized(embeddings, groups, n_components=2):\n",
    "    \"\"\"\n",
    "    Remove principal components that capture image-level variation,\n",
    "    specially adapted for L2-normalized embeddings.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    embeddings : numpy array of shape (n_samples, n_features)\n",
    "        L2-normalized embedding vectors\n",
    "    groups : numpy array of shape (n_samples,)\n",
    "        groups ID for each tile to remove the effect of\n",
    "    n_components : int\n",
    "        Number of principal components to remove\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    corrected_embeddings : numpy array of shape (n_samples, n_features)\n",
    "        Embeddings with image-level PCs removed\n",
    "    \"\"\"\n",
    "    unique_group_ids = np.unique(groups)\n",
    "    \n",
    "    # Compute image means\n",
    "    group_means = np.zeros((len(unique_group_ids), embeddings.shape[1]))\n",
    "    for i, group_id in enumerate(unique_group_ids):\n",
    "        mask = groups == group_id\n",
    "        # For L2-normalized vectors, take the mean and re-normalize\n",
    "        mean_vector = embeddings[mask].mean(axis=0)\n",
    "        group_means[i] = mean_vector / np.linalg.norm(mean_vector)\n",
    "    \n",
    "    # Compute PCA on image means to identify image-specific directions\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(group_means)\n",
    "    image_pcs = pca.components_\n",
    "    \n",
    "    # For each embedding, remove projection onto image PCs\n",
    "    corrected_embeddings = embeddings.copy()\n",
    "    for i in range(len(embeddings)):\n",
    "        embedding = embeddings[i]\n",
    "        \n",
    "        # Remove projections onto image PCs\n",
    "        for pc in image_pcs:\n",
    "            # Calculate projection\n",
    "            proj = np.dot(embedding, pc) * pc\n",
    "            # Subtract projection\n",
    "            embedding = embedding - proj\n",
    "            \n",
    "        # Renormalize to unit length\n",
    "        corrected_embeddings[i] = embedding / np.linalg.norm(embedding)\n",
    "    \n",
    "    return corrected_embeddings\n",
    "\n",
    "pca_embeddings = remove_image_pcs_for_normalized(embeddings, metadata['patient_id'].values, n_components=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f74cc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_deconfounding_parameters(embeddings, metadata, deconf_type='patient'):\n",
    "    \"\"\"\n",
    "    Analyze effect of different parameters for orthogonal deconfounding.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    embeddings : numpy array\n",
    "        Original L2-normalized embeddings\n",
    "    metadata : DataFrame\n",
    "        Metadata containing class_name, patient_id and image_id\n",
    "    deconf_type : str\n",
    "        Whether to analyze 'patient' or 'image' deconfounding\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "    # Set up parameter ranges\n",
    "    n_comp_range = [5, 10, 20, 30, 40, 45, 60, 120, 240, 360, 481]  # None means use all components\n",
    "    c_values = [0.01, 0.1, 1.0, 10.0]  # Regularization strength\n",
    "    \n",
    "    # Set up group column\n",
    "    if deconf_type == 'patient':\n",
    "        group_col = 'patient_id'\n",
    "        other_col = 'image_id'\n",
    "    else:  # image\n",
    "        group_col = 'image_id'\n",
    "        other_col = 'patient_id'\n",
    "    \n",
    "    # Prepare data for evaluation\n",
    "    class_labels = metadata['class_name'].values\n",
    "    group_labels = metadata[group_col].values\n",
    "    other_labels = metadata[other_col].values\n",
    "    \n",
    "    # Set up classifier\n",
    "    class_clf = LogisticRegression(solver='lbfgs', max_iter=2000)\n",
    "    group_clf = LogisticRegression(solver='lbfgs', max_iter=2000)\n",
    "    other_clf = LogisticRegression(solver='lbfgs', max_iter=2000)\n",
    "    \n",
    "    # Results storage\n",
    "    results = []\n",
    "    \n",
    "    # Original baseline\n",
    "    orig_class = cross_val_score(class_clf, embeddings, class_labels, cv=5).mean()\n",
    "    orig_group = cross_val_score(group_clf, embeddings, group_labels, cv=5).mean()\n",
    "    orig_other = cross_val_score(other_clf, embeddings, other_labels, cv=5).mean()\n",
    "    \n",
    "    print(f\"Original embeddings:\")\n",
    "    print(f\"  Class prediction: {orig_class:.4f}\")\n",
    "    print(f\"  {group_col} prediction: {orig_group:.4f}\")\n",
    "    print(f\"  {other_col} prediction: {orig_other:.4f}\")\n",
    "    print(f\"  Class-to-{group_col} ratio: {orig_class/orig_group:.4f}\")\n",
    "    \n",
    "    # Evaluate different parameter combinations\n",
    "    for c_val in c_values:\n",
    "        deconfounder = OrthogonalDeconfounding(C=c_val)\n",
    "        deconfounded = deconfounder.fit(embeddings, groups=group_labels)\n",
    "        for n_comp in n_comp_range:\n",
    "            # Skip invalid combinations\n",
    "            if n_comp is not None and n_comp > len(np.unique(group_labels)):\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\nEvaluating: n_components={n_comp}, C={c_val}\")\n",
    "            \n",
    "            # Create deconfounder\n",
    "            \n",
    "            try:\n",
    "                # Apply deconfounding\n",
    "                deconfounded = deconfounder.transform(embeddings,  n_components=n_comp)\n",
    "                \n",
    "                # Evaluate\n",
    "                class_score = cross_val_score(class_clf, deconfounded, class_labels, cv=5).mean()\n",
    "                group_score = cross_val_score(group_clf, deconfounded, group_labels, cv=5).mean()\n",
    "                other_score = cross_val_score(other_clf, deconfounded, other_labels, cv=5).mean()\n",
    "                \n",
    "                print(f\"  Class prediction: {class_score:.4f}\")\n",
    "                print(f\"  {group_col} prediction: {group_score:.4f}\")\n",
    "                print(f\"  {other_col} prediction: {other_score:.4f}\")\n",
    "                print(f\"  Class-to-{group_col} ratio: {class_score/group_score:.4f}\")\n",
    "                \n",
    "                # Store results\n",
    "                results.append({\n",
    "                    'n_components': n_comp,\n",
    "                    'C': c_val,\n",
    "                    'class_score': class_score,\n",
    "                    'group_score': group_score,\n",
    "                    'other_score': other_score,\n",
    "                    'class_group_ratio': class_score / group_score,\n",
    "                    'class_drop': orig_class - class_score,\n",
    "                    'group_drop': orig_group - group_score\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error with parameters n_components={n_comp}, C={c_val}: {e}\")\n",
    "    \n",
    "    # Convert to dataframe\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.lineplot(data=results_df, x='n_components', y='class_score', hue='C', marker='o')\n",
    "    plt.axhline(y=orig_class, linestyle='--', color='black', label='Original')\n",
    "    plt.title('Class Prediction Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.legend(title='C value')\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.lineplot(data=results_df, x='n_components', y='group_score', hue='C', marker='o')\n",
    "    plt.axhline(y=orig_group, linestyle='--', color='black', label='Original')\n",
    "    plt.axhline(y=1.0/len(np.unique(group_labels)), linestyle=':', color='red', \n",
    "                label=f'Random ({1.0/len(np.unique(group_labels)):.4f})')\n",
    "    plt.title(f'{group_col.capitalize()} Prediction Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.legend(title='C value')\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.lineplot(data=results_df, x='n_components', y='class_group_ratio', hue='C', marker='o')\n",
    "    plt.axhline(y=orig_class/orig_group, linestyle='--', color='black', label='Original')\n",
    "    plt.title('Class-to-Group Ratio (higher is better)')\n",
    "    plt.ylabel('Ratio')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.legend(title='C value')\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    # Create a scatterplot showing the tradeoff\n",
    "    for c_val in c_values:\n",
    "        subset = results_df[results_df['C'] == c_val]\n",
    "        plt.scatter(subset['group_drop'], subset['class_drop'], \n",
    "                   label=f'C={c_val}', s=80, alpha=0.7)\n",
    "        # Add n_components as annotations\n",
    "        for _, row in subset.iterrows():\n",
    "            plt.annotate(f\"{row['n_components']}\", \n",
    "                        (row['group_drop'], row['class_drop']), \n",
    "                        fontsize=9)\n",
    "    \n",
    "    # Add lines to divide the plot into quadrants\n",
    "    plt.axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "    plt.axvline(x=0, color='black', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Add a diagonal line representing equal drops\n",
    "    max_drop = max(results_df['group_drop'].max(), results_df['class_drop'].max())\n",
    "    plt.plot([0, max_drop], [0, max_drop], 'k--', alpha=0.3)\n",
    "    \n",
    "    # Add better labels\n",
    "    plt.xlabel(f'Drop in {group_col} prediction accuracy')\n",
    "    plt.ylabel('Drop in class prediction accuracy')\n",
    "    plt.title('Tradeoff between group removal and class preservation')\n",
    "    \n",
    "    # Add a legend\n",
    "    plt.legend(title='C value')\n",
    "    \n",
    "    # Optimal area shading - we want high group drop but low class drop\n",
    "    plt.fill_between([0, max_drop], [0, 0], [0, max_drop], \n",
    "                    color='green', alpha=0.1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find best parameters based on highest ratio\n",
    "    best_row = results_df.loc[results_df['class_group_ratio'].idxmax()]\n",
    "    print(\"\\nBest parameters based on highest class-to-group ratio:\")\n",
    "    print(f\"n_components: {best_row['n_components']}, C: {best_row['C']}\")\n",
    "    print(f\"Class accuracy: {best_row['class_score']:.4f}\")\n",
    "    print(f\"{group_col} accuracy: {best_row['group_score']:.4f}\")\n",
    "    print(f\"Class-to-{group_col} ratio: {best_row['class_group_ratio']:.4f}\")\n",
    "    \n",
    "    # Find best parameters based on group removal with least class drop\n",
    "    # Normalize both drops to [0,1] range\n",
    "    results_df['norm_group_drop'] = results_df['group_drop'] / results_df['group_drop'].max()\n",
    "    results_df['norm_class_drop'] = results_df['class_drop'] / results_df['class_drop'].max()\n",
    "    \n",
    "    # Create a combined score: maximize group drop, minimize class drop\n",
    "    results_df['removal_score'] = results_df['norm_group_drop'] - results_df['norm_class_drop']\n",
    "    \n",
    "    best_removal_row = results_df.loc[results_df['removal_score'].idxmax()]\n",
    "    print(\"\\nBest parameters based on optimal removal:\")\n",
    "    print(f\"n_components: {best_removal_row['n_components']}, C: {best_removal_row['C']}\")\n",
    "    print(f\"Class accuracy: {best_removal_row['class_score']:.4f}\")\n",
    "    print(f\"{group_col} accuracy: {best_removal_row['group_score']:.4f}\")\n",
    "    print(f\"Class-to-{group_col} ratio: {best_removal_row['class_group_ratio']:.4f}\")\n",
    "    \n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b53ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run analysis for patient deconfounding\n",
    "patient_results = analyze_deconfounding_parameters(embeddings, metadata, deconf_type='patient')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c598268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis for image deconfounding\n",
    "image_results = analyze_deconfounding_parameters(embeddings, metadata, deconf_type='image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba093963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchpl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
